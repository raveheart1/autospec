feature:
    branch: "101-e2e-mock-testing"
    created: "2026-01-16"
    status: "Completed"
    completed_at: 2026-01-16T17:48:45Z
    input: "E2E Testing with Mock Claude Binary - True end-to-end testing of autospec CLI commands without API costs or risk of invoking real claude CLI. Create mock claude script, E2E test fixtures, test environment setup with PATH isolation, and comprehensive test coverage for all workflow stages."
user_stories:
    - id: "US-001"
      title: "Developer runs E2E tests safely"
      priority: "P1"
      as_a: "autospec developer"
      i_want: "to run end-to-end tests that exercise the full autospec CLI without invoking the real Claude API"
      so_that: "I can verify the complete command-to-artifact chain works correctly without incurring API costs or risking real API calls"
      why_this_priority: "Core value proposition - without this, E2E testing is either expensive or risky"
      independent_test: "Run go test -tags=e2e and verify mock claude is invoked instead of real claude"
      acceptance_scenarios:
        - given: "E2E test environment is set up with mock claude in PATH"
          when: "I run any autospec stage command"
          then: "The mock claude binary is invoked and returns appropriate YAML responses"
        - given: "E2E tests are running"
          when: "The test environment is active"
          then: "No ANTHROPIC_API_KEY is present in the environment and real claude is not in PATH"
    - id: "US-002"
      title: "Developer tests individual stages"
      priority: "P1"
      as_a: "autospec developer"
      i_want: "to test each workflow stage (specify, plan, tasks, implement) independently"
      so_that: "I can verify each stage produces valid artifacts without running the full workflow"
      why_this_priority: "Essential for granular testing and debugging of individual stages"
      independent_test: "Run TestE2E_Specify and verify spec.yaml is created with valid structure"
      acceptance_scenarios:
        - given: "Mock claude returns valid spec.yaml content"
          when: "I run autospec specify with a feature description"
          then: "A valid spec.yaml file is created in the feature directory"
        - given: "Mock claude returns valid plan.yaml content"
          when: "I run autospec plan in a feature directory with spec.yaml"
          then: "A valid plan.yaml file is created"
        - given: "Mock claude returns valid tasks.yaml content"
          when: "I run autospec tasks in a feature directory with plan.yaml"
          then: "A valid tasks.yaml file is created with phases"
    - id: "US-003"
      title: "Developer tests full workflows"
      priority: "P1"
      as_a: "autospec developer"
      i_want: "to test complete workflows like prep and run"
      so_that: "I can verify multi-stage orchestration works end-to-end"
      why_this_priority: "Workflows are the primary user interface and must work correctly"
      independent_test: "Run TestE2E_PrepWorkflow and verify all three artifacts are created in sequence"
      acceptance_scenarios:
        - given: "Mock claude is configured and constitution exists"
          when: "I run autospec prep with a feature description"
          then: "spec.yaml, plan.yaml, and tasks.yaml are all created in the correct order"
        - given: "Mock claude is configured and constitution exists"
          when: "I run autospec run -a with a feature description"
          then: "All artifacts are created and implementation completes successfully"
    - id: "US-004"
      title: "Developer tests error handling"
      priority: "P2"
      as_a: "autospec developer"
      i_want: "to test that error conditions are handled correctly"
      so_that: "I can verify autospec fails gracefully with appropriate error messages"
      why_this_priority: "Important for user experience but secondary to happy path testing"
      independent_test: "Run TestE2E_MissingConstitution and verify error message and non-zero exit"
      acceptance_scenarios:
        - given: "No constitution.yaml exists in the test environment"
          when: "I run any workflow stage command"
          then: "An error message about missing constitution is shown and exit code is non-zero"
        - given: "No tasks.yaml exists"
          when: "I run autospec implement"
          then: "An error about missing prerequisite is shown and exit code is non-zero"
        - given: "Mock claude is configured to return exit code 1"
          when: "I run any autospec stage command"
          then: "The error is propagated correctly with appropriate exit code"
    - id: "US-005"
      title: "Developer runs E2E tests in CI"
      priority: "P2"
      as_a: "CI/CD pipeline maintainer"
      i_want: "E2E tests to be isolated via build tag so they only run when explicitly requested"
      so_that: "Normal test runs are fast and E2E tests can be run separately as needed"
      why_this_priority: "CI efficiency is important but secondary to core functionality"
      independent_test: "Run go test ./... and verify E2E tests are not executed; run go test -tags=e2e ./tests/e2e/... and verify they execute"
      acceptance_scenarios:
        - given: "E2E tests are marked with //go:build e2e build tag"
          when: "I run go test ./... without the e2e tag"
          then: "E2E tests are skipped"
        - given: "E2E tests exist"
          when: "I run go test -tags=e2e ./tests/e2e/..."
          then: "E2E tests execute and complete"
requirements:
    functional:
        - id: "FR-001"
          description: "MUST create a mock claude script that detects which stage is being invoked from prompt content and returns appropriate YAML"
          testable: true
          acceptance_criteria: "Mock script parses prompts and returns valid responses for specify, plan, tasks, implement, and constitution stages"
        - id: "FR-002"
          description: "MUST provide E2E test fixtures with valid YAML responses for each stage in testdata/e2e/responses/"
          testable: true
          acceptance_criteria: "Directory contains spec.yaml, plan.yaml, tasks.yaml, constitution.yaml, and implement.txt with valid content"
        - id: "FR-003"
          description: "MUST implement E2E test environment setup that isolates PATH to only include mock binaries"
          testable: true
          acceptance_criteria: "E2EEnv helper creates temp directory, copies mock as 'claude', builds autospec, and sets PATH to only include mock dir"
        - id: "FR-004"
          description: "MUST ensure no ANTHROPIC_API_KEY is present in E2E test environment"
          testable: true
          acceptance_criteria: "E2EEnv.Run() sets environment without API key; tests fail if real API is somehow called"
        - id: "FR-005"
          description: "MUST implement basic stage tests for specify, plan, tasks, and implement commands"
          testable: true
          acceptance_criteria: "TestE2E_Specify, TestE2E_Plan, TestE2E_Tasks, and TestE2E_Implement all pass and verify correct artifacts"
        - id: "FR-006"
          description: "MUST implement workflow tests for prep and run commands"
          testable: true
          acceptance_criteria: "TestE2E_PrepWorkflow and TestE2E_FullWorkflow verify complete artifact chains"
        - id: "FR-007"
          description: "MUST implement error handling tests for missing prerequisites and mock failures"
          testable: true
          acceptance_criteria: "TestE2E_MissingConstitution, TestE2E_MissingPrereq, and TestE2E_MockFailure verify proper error handling"
        - id: "FR-008"
          description: "MUST use //go:build e2e build tag to isolate E2E tests from normal test runs"
          testable: true
          acceptance_criteria: "E2E tests only run with -tags=e2e flag"
        - id: "FR-009"
          description: "MUST create E2EEnv helper in internal/testutil/e2e.go for test environment setup and command execution"
          testable: true
          acceptance_criteria: "Helper provides setupE2EEnv() and Run() methods with correct environment isolation"
        - id: "FR-010"
          description: "MUST run make test && make fmt && make lint && make build with exit code 0"
          testable: true
          acceptance_criteria: "All commands pass without errors"
    non_functional:
        - id: "NFR-001"
          category: "performance"
          description: "E2E test suite MUST complete in under 30 seconds"
          measurable_target: "Total E2E test execution time < 30 seconds with mock responses"
        - id: "NFR-002"
          category: "reliability"
          description: "E2E tests MUST have zero risk of invoking real Claude API"
          measurable_target: "PATH isolation prevents any possibility of real claude invocation"
        - id: "NFR-003"
          category: "usability"
          description: "E2E tests MUST work on both Linux and macOS"
          measurable_target: "Tests pass on both platforms without modification"
        - id: "NFR-004"
          category: "code_quality"
          description: "Functions MUST be under 40 lines"
          measurable_target: "No function exceeds 40 lines of code"
        - id: "NFR-005"
          category: "code_quality"
          description: "Errors MUST be wrapped with context using fmt.Errorf"
          measurable_target: "All error returns use fmt.Errorf('doing X: %w', err) pattern"
        - id: "NFR-006"
          category: "code_quality"
          description: "Tests MUST use map-based table test pattern"
          measurable_target: "All test files use tests := map[string]struct{...} pattern"
success_criteria:
    measurable_outcomes:
        - id: "SC-001"
          description: "Developers can run full E2E tests of autospec CLI without API costs"
          metric: "Number of real API calls during E2E test suite"
          target: "Zero API calls"
        - id: "SC-002"
          description: "E2E tests verify complete command-to-artifact chain for all stages"
          metric: "Stage coverage"
          target: "100% coverage of specify, plan, tasks, implement, and constitution stages"
        - id: "SC-003"
          description: "E2E tests complete quickly enough for practical use"
          metric: "Total test execution time"
          target: "Under 30 seconds"
        - id: "SC-004"
          description: "E2E tests do not impact normal test workflow"
          metric: "Test isolation"
          target: "E2E tests excluded from default go test ./... runs"
        - id: "SC-005"
          description: "Error handling coverage ensures graceful failures"
          metric: "Error scenario coverage"
          target: "Tests exist for missing constitution, missing prerequisites, and mock failures"
key_entities:
    - name: "MockClaude"
      description: "A shell script that mimics the claude CLI by detecting stage from prompt and returning valid YAML"
      attributes:
        - "Stage detection via prompt content patterns"
        - "Returns appropriate YAML responses from fixtures"
        - "Configurable exit codes for error simulation"
    - name: "E2EEnv"
      description: "Test helper struct that manages isolated test environment"
      attributes:
        - "Temp directory for test isolation"
        - "PATH containing only mock and autospec binaries"
        - "Environment without ANTHROPIC_API_KEY"
        - "Run() method for executing commands"
    - name: "ResponseFixtures"
      description: "Pre-defined valid YAML responses for each stage"
      attributes:
        - "spec.yaml with valid spec structure"
        - "plan.yaml with valid plan structure"
        - "tasks.yaml with valid task phases"
        - "constitution.yaml with valid constitution"
        - "implement.txt with success message"
edge_cases:
    - scenario: "Mock script receives unrecognized prompt pattern"
      expected_behavior: "Return generic success response and exit 0 to avoid false test failures"
    - scenario: "Binary build fails during test setup"
      expected_behavior: "Test fails immediately with clear error about build failure"
    - scenario: "Mock script file is missing from testdata"
      expected_behavior: "Test fails fast with error about missing mock binary"
    - scenario: "Temp directory creation fails"
      expected_behavior: "Test fails with clear error about environment setup failure"
    - scenario: "Multiple E2E tests run in parallel"
      expected_behavior: "Each test has isolated temp directory, no interference between tests"
    - scenario: "Mock returns malformed YAML"
      expected_behavior: "Autospec validation catches the error and reports appropriately"
assumptions:
    - "Go build toolchain is available in the test environment"
    - "Shell script execution is available (bash or compatible shell)"
    - "Temp directory creation is reliable on test platforms"
    - "File permissions (0755 for executables) work correctly on test platforms"
    - "Tests will primarily run on Unix-like systems (Linux/macOS)"
    - "The mock-claude.sh script can detect stages via simple string matching in prompts"
constraints:
    - "Windows support is out of scope for initial implementation (would require .bat equivalent)"
    - "E2E tests require building the autospec binary, adding time to test runs"
    - "Response fixtures must be kept in sync with artifact schema changes"
    - "Build tag isolation means E2E tests are not run by default"
out_of_scope:
    - "Windows support for E2E testing"
    - "Integration with actual Claude API for any test scenarios"
    - "Performance benchmarking beyond the 30-second completion target"
    - "Testing of shell completion functionality"
    - "Testing of MCP or external tool integrations"
    - "Automated fixture regeneration when schemas change"
    - "Parallel execution of E2E tests (each test runs sequentially)"
_meta:
    version: "1.0.0"
    generator: "autospec"
    generator_version: "autospec 0.9.0"
    created: "2026-01-16T10:16:14Z"
    artifact_type: "spec"
