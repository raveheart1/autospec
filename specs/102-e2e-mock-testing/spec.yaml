feature:
    branch: "102-e2e-mock-testing"
    created: "2026-01-16"
    status: "Completed"
    completed_at: 2026-01-16T19:58:38Z
    input: "create a comprehensive spec for .dev/tasks/e2e-test-coverage-analysis.md"
user_stories:
    - id: "US-001"
      title: "Safety Verification Tests"
      priority: "P1"
      as_a: "developer running E2E tests"
      i_want: "tests that verify no real API calls are made and environment is properly isolated"
      so_that: "I can run tests without incurring API costs or affecting production systems"
      why_this_priority: "Non-negotiable safety requirement - real API calls must never occur in tests"
      independent_test: "Run E2E tests and verify ANTHROPIC_API_KEY is not accessible and only mock binaries are in PATH"
      acceptance_scenarios:
        - given: "E2E test environment is initialized"
          when: "any E2E test runs"
          then: "ANTHROPIC_API_KEY and OPENAI_API_KEY are not in environment"
        - given: "E2E test environment is initialized"
          when: "PATH is checked"
          then: "only mock binaries (mock-claude.sh, mock-opencode.sh) are accessible, not real CLI binaries"
        - given: "E2E test runs any command"
          when: "command writes state files"
          then: "state files are written to temp directory, not ~/.autospec/ or ~/.config/autospec/"
        - given: "E2E test runs git operations"
          when: "git commands execute"
          then: "operations use temp git repo with GIT_CONFIG_GLOBAL=/dev/null"
    - id: "US-002"
      title: "OpenCode Agent Mock Support"
      priority: "P1"
      as_a: "developer testing OpenCode agent workflows"
      i_want: "a mock-opencode.sh script that simulates OpenCode CLI behavior"
      so_that: "I can test autospec workflows with the OpenCode agent preset without real API calls"
      why_this_priority: "OpenCode is a supported agent preset with zero E2E coverage - critical gap"
      independent_test: "Run autospec with --agent opencode and verify mock-opencode.sh is invoked and generates correct artifacts"
      acceptance_scenarios:
        - given: "mock-opencode.sh exists and is executable"
          when: "autospec runs with --agent opencode"
          then: "mock-opencode.sh is invoked instead of real opencode binary"
        - given: "mock-opencode.sh receives a specify command"
          when: "command contains /autospec.specify"
          then: "mock generates valid spec.yaml artifact"
        - given: "MOCK_EXIT_CODE environment variable is set"
          when: "mock-opencode.sh runs"
          then: "mock exits with the specified exit code"
        - given: "MOCK_CALL_LOG environment variable is set"
          when: "mock-opencode.sh runs"
          then: "command invocation is logged to specified file"
    - id: "US-003"
      title: "Exit Code Verification"
      priority: "P1"
      as_a: "developer testing error handling"
      i_want: "E2E tests that verify all 6 documented exit codes (0-5)"
      so_that: "I can confirm autospec returns correct exit codes for all scenarios"
      why_this_priority: "Exit codes are part of the CLI contract - must be verified for automation/scripting"
      independent_test: "Run tests for each exit code scenario and verify correct code is returned"
      acceptance_scenarios:
        - given: "workflow completes successfully"
          when: "autospec exits"
          then: "exit code is 0"
        - given: "artifact validation fails"
          when: "autospec exits"
          then: "exit code is 1"
        - given: "retry limit is exhausted"
          when: "autospec exits"
          then: "exit code is 2"
        - given: "invalid arguments are provided"
          when: "autospec exits"
          then: "exit code is 3"
        - given: "required dependency is missing"
          when: "autospec exits"
          then: "exit code is 4"
        - given: "command times out"
          when: "autospec exits"
          then: "exit code is 5"
    - id: "US-004"
      title: "Implement Command Flag Testing"
      priority: "P1"
      as_a: "developer using implement command variants"
      i_want: "E2E tests covering all implement command execution modes"
      so_that: "I can be confident phase-based and task-based execution work correctly"
      why_this_priority: "implement has the most flags (8+) with zero flag coverage - highest gap"
      independent_test: "Run implement with each execution mode flag and verify correct behavior"
      acceptance_scenarios:
        - given: "tasks.yaml exists with multiple phases"
          when: "autospec implement --phases runs"
          then: "each phase runs in a separate session"
        - given: "tasks.yaml exists with phase 2"
          when: "autospec implement --phase 2 runs"
          then: "only phase 2 tasks execute"
        - given: "tasks.yaml exists and previous run was interrupted"
          when: "autospec implement --from-phase 2 runs"
          then: "execution resumes from phase 2"
        - given: "tasks.yaml exists with multiple tasks"
          when: "autospec implement --tasks runs"
          then: "each task runs in a separate session"
        - given: "tasks.yaml exists with task T003"
          when: "autospec implement --from-task T003 runs"
          then: "execution resumes from task T003"
    - id: "US-005"
      title: "Run Command Coverage"
      priority: "P1"
      as_a: "developer using the run command"
      i_want: "E2E tests covering all run command stage selection flags"
      so_that: "I can verify stage selection and optional stages work correctly"
      why_this_priority: "run command has zero E2E coverage despite being primary entry point"
      independent_test: "Run with various flag combinations and verify correct stages execute"
      acceptance_scenarios:
        - given: "constitution exists"
          when: "autospec run -spti 'feature' executes"
          then: "specify, plan, tasks, and implement all run in order"
        - given: "constitution exists"
          when: "autospec run -a 'feature' executes"
          then: "behavior is identical to -spti"
        - given: "constitution exists"
          when: "autospec run -itps 'feature' executes"
          then: "stages run in correct order (specify, plan, tasks, implement) regardless of flag order"
        - given: "spec.yaml exists"
          when: "autospec run -p executes"
          then: "only plan stage runs"
        - given: "no spec.yaml exists"
          when: "autospec run -p executes"
          then: "command fails with error mentioning 'spec'"
    - id: "US-006"
      title: "Phase Ordering Property Tests"
      priority: "P2"
      as_a: "developer maintaining autospec"
      i_want: "property-based tests that verify flag order independence"
      so_that: "I can be confident that flag permutations don't affect execution order"
      why_this_priority: "Important for CLI robustness but core flag tests are higher priority"
      independent_test: "Generate random flag permutations and verify same artifacts are created"
      acceptance_scenarios:
        - given: "any permutation of flags -s, -p, -t, -i"
          when: "autospec run with that permutation executes"
          then: "stages always execute in order: specify, plan, tasks, implement"
        - given: "subset of flags specified (e.g., -sp)"
          when: "autospec run executes"
          then: "only specified stages run and only their artifacts are created"
        - given: "a stage is requested without its prerequisite"
          when: "autospec run executes"
          then: "error message mentions the missing prerequisite"
    - id: "US-007"
      title: "Interactive Stage Mock Support"
      priority: "P2"
      as_a: "developer testing optional workflow stages"
      i_want: "mock support for clarify, checklist, and analyze commands"
      so_that: "I can test all workflow stages without real API calls"
      why_this_priority: "Optional stages have zero E2E coverage but are less frequently used"
      independent_test: "Run clarify, checklist, analyze and verify mock generates appropriate artifacts"
      acceptance_scenarios:
        - given: "spec.yaml exists"
          when: "autospec clarify runs"
          then: "mock-claude.sh detects /autospec.clarify and updates spec.yaml with clarifications"
        - given: "spec.yaml exists"
          when: "autospec checklist runs"
          then: "mock-claude.sh detects /autospec.checklist and generates checklist.yaml"
        - given: "spec.yaml, plan.yaml, tasks.yaml exist"
          when: "autospec analyze runs"
          then: "mock-claude.sh detects /autospec.analyze and generates analysis.yaml"
    - id: "US-008"
      title: "Global Flag Testing"
      priority: "P2"
      as_a: "developer using global autospec flags"
      i_want: "E2E tests for global flags like --max-retries, --agent, --config"
      so_that: "I can verify global flags work correctly across all commands"
      why_this_priority: "Global flags affect all commands but have zero coverage"
      independent_test: "Run commands with global flags and verify they take effect"
      acceptance_scenarios:
        - given: "command is run with --max-retries 5"
          when: "mock returns failure"
          then: "command retries up to 5 times before failing"
        - given: "command is run with --agent opencode"
          when: "command executes"
          then: "mock-opencode.sh is invoked instead of mock-claude.sh"
        - given: "custom config file exists"
          when: "command is run with --config ./custom.yml"
          then: "settings from custom.yml are used"
        - given: "command is run with --skip-preflight"
          when: "command executes"
          then: "preflight checks are skipped"
    - id: "US-009"
      title: "Error Scenario Coverage"
      priority: "P2"
      as_a: "developer handling error cases"
      i_want: "E2E tests for various error scenarios"
      so_that: "I can verify error messages are clear and exit codes are correct"
      why_this_priority: "Error handling affects user experience but success paths are higher priority"
      independent_test: "Trigger each error scenario and verify error message and exit code"
      acceptance_scenarios:
        - given: "implement command is run"
          when: "tasks.yaml does not exist"
          then: "exit code is 1 and error message mentions 'tasks'"
        - given: "implement --phase 99 is run"
          when: "phase 99 does not exist"
          then: "exit code is 1 and error message mentions invalid phase"
        - given: "agent binary is removed from PATH"
          when: "any workflow command runs"
          then: "exit code is 4 and error message mentions missing binary"
        - given: "config file has invalid YAML"
          when: "any command runs"
          then: "exit code is 1 and error message mentions parse error"
    - id: "US-010"
      title: "Utility Command Testing"
      priority: "P2"
      as_a: "developer using utility commands"
      i_want: "E2E tests for status, history, clean, view, and ck commands"
      so_that: "I can verify utility commands work correctly"
      why_this_priority: "100% command coverage requires all utility commands tested"
      independent_test: "Run each utility command and verify output/behavior"
      acceptance_scenarios:
        - given: "a spec exists"
          when: "autospec status runs"
          then: "status shows current spec state"
        - given: "commands have been run"
          when: "autospec history runs"
          then: "history shows recent command executions"
        - given: "a spec.yaml exists"
          when: "autospec view spec runs"
          then: "spec contents are displayed"
        - given: "a valid YAML file exists"
          when: "autospec ck file.yaml runs"
          then: "validation passes"
    - id: "US-011"
      title: "Config, Admin, Worktree, and DAG Command Testing"
      priority: "P2"
      as_a: "developer using autospec"
      i_want: "E2E tests for all remaining command categories (config, admin, worktree, dag)"
      so_that: "I can verify 100% of CLI commands have test coverage"
      why_this_priority: "100% command coverage is now a requirement per clarification"
      independent_test: "Run each command category and verify basic functionality"
      acceptance_scenarios:
        - given: "a fresh directory"
          when: "autospec init runs"
          then: "config files are created"
        - given: "config exists"
          when: "autospec config show runs"
          then: "current configuration is displayed"
        - given: "dependencies are installed"
          when: "autospec doctor runs"
          then: "dependency status is shown"
        - given: "a git repo exists"
          when: "autospec worktree list runs"
          then: "worktree list is displayed (empty or populated)"
        - given: "tasks.yaml with dependencies exists"
          when: "autospec dag validate runs"
          then: "DAG validation result is shown"
        - given: "shell is bash"
          when: "autospec completion bash runs"
          then: "bash completion script is output"
        - given: "commands are installed"
          when: "autospec commands check runs"
          then: "command installation status is shown"
requirements:
    functional:
        - id: "FR-001"
          description: "MUST never invoke real claude or opencode CLI binaries during E2E tests"
          testable: true
          acceptance_criteria: "E2EEnv isolates PATH to only include mock binaries; verification test confirms real binaries are not accessible"
        - id: "FR-002"
          description: "MUST sanitize ANTHROPIC_API_KEY, OPENAI_API_KEY, and similar environment variables in E2E tests"
          testable: true
          acceptance_criteria: "E2EEnv removes all API keys from environment; verification test confirms no API keys are accessible"
        - id: "FR-003"
          description: "MUST use temp directories for all state files (retry.json, config.yml, history.json)"
          testable: true
          acceptance_criteria: "E2EEnv sets XDG_CONFIG_HOME, HOME, AUTOSPEC_STATE_DIR to temp directory; no writes to real home directory"
        - id: "FR-004"
          description: "MUST isolate git operations with GIT_CONFIG_GLOBAL=/dev/null and temp repos"
          testable: true
          acceptance_criteria: "E2EEnv creates fresh git repo in temp dir with minimal config; no interaction with real .git/"
        - id: "FR-005"
          description: "MUST provide mock-opencode.sh script that mimics OpenCode CLI behavior"
          testable: true
          acceptance_criteria: "mock-opencode.sh exists, is executable, parses -m flag, generates artifacts, supports MOCK_* variables"
        - id: "FR-006"
          description: "MUST verify all 6 exit codes (0-5) in E2E tests"
          testable: true
          acceptance_criteria: "Dedicated test cases exist for each exit code with appropriate mock configurations"
        - id: "FR-007"
          description: "MUST test all implement command execution modes (--phases, --tasks, --phase N, --from-phase, --from-task)"
          testable: true
          acceptance_criteria: "E2E tests exist for each execution mode with verified correct behavior"
        - id: "FR-008"
          description: "MUST test run command stage selection flags (-s, -p, -t, -i, -a) and optional stage flags (-n, -r, -l, -z)"
          testable: true
          acceptance_criteria: "E2E tests exist for stage selection with verified correct stage execution order"
        - id: "FR-009"
          description: "MUST enhance mock-claude.sh to support /autospec.clarify, /autospec.checklist, /autospec.analyze"
          testable: true
          acceptance_criteria: "Mock script generates appropriate artifacts for each slash command"
        - id: "FR-010"
          description: "MUST test global flags --max-retries, --agent, --config, --skip-preflight across commands"
          testable: true
          acceptance_criteria: "E2E tests verify each global flag affects command behavior correctly"
        - id: "FR-011"
          description: "SHOULD test phase ordering invariant (flag order does not affect execution order)"
          testable: true
          acceptance_criteria: "Property-based test verifies -spti and -itps produce same results"
        - id: "FR-012"
          description: "SHOULD test error scenarios with clear error messages and correct exit codes"
          testable: true
          acceptance_criteria: "E2E tests for missing prerequisites, invalid flags, missing binaries verify error messages"
        - id: "FR-013"
          description: "MUST test utility commands (status, history, clean, view, ck)"
          testable: true
          acceptance_criteria: "E2E tests exist for utility commands with basic functionality verification"
        - id: "FR-014"
          description: "MUST make tests pass with 'make test && make fmt && make lint && make build' all exit 0"
          testable: true
          acceptance_criteria: "All quality gates pass after implementation"
        - id: "FR-015"
          description: "MUST provide E2E test coverage for 100% of CLI commands (all 35+ commands)"
          testable: true
          acceptance_criteria: "Every command in autospec CLI has at least one E2E test covering basic functionality"
        - id: "FR-016"
          description: "MUST test config commands (init, config show/set/get/toggle/keys/sync, migrate, doctor)"
          testable: true
          acceptance_criteria: "E2E tests exist for all config-related commands with basic functionality verification"
        - id: "FR-017"
          description: "MUST test worktree commands (create, list, remove, prune, setup)"
          testable: true
          acceptance_criteria: "E2E tests exist for all worktree subcommands with basic functionality verification"
        - id: "FR-018"
          description: "MUST test dag commands (run, status, resume, logs, reset, validate, watch)"
          testable: true
          acceptance_criteria: "E2E tests exist for all dag subcommands with basic functionality verification"
        - id: "FR-019"
          description: "MUST test admin commands (commands check/info/install, completion bash/zsh/fish/powershell/install, uninstall)"
          testable: true
          acceptance_criteria: "E2E tests exist for all admin commands with basic functionality verification"
    non_functional:
        - id: "NFR-001"
          category: "performance"
          description: "E2E tests should complete within CI timeout limits with parallelization"
          measurable_target: "Individual E2E tests complete in under 10 seconds; full E2E suite in under 5 minutes using parallel test execution"
        - id: "NFR-002"
          category: "reliability"
          description: "E2E tests must be deterministic and not flaky"
          measurable_target: "100% pass rate on 10 consecutive runs with no intermittent failures"
        - id: "NFR-003"
          category: "reliability"
          description: "E2E tests must run identically on Linux, macOS, and CI environments"
          measurable_target: "Tests pass on local Linux/macOS and GitHub Actions without modification"
        - id: "NFR-004"
          category: "code_quality"
          description: "Functions must be under 40 lines"
          measurable_target: "No function exceeds 40 lines"
        - id: "NFR-005"
          category: "code_quality"
          description: "Errors must be wrapped with context"
          measurable_target: "All errors use fmt.Errorf('doing X: %w', err) pattern"
        - id: "NFR-006"
          category: "code_quality"
          description: "Tests must use map-based table test pattern"
          measurable_target: "All table tests use map[string]struct pattern"
        - id: "NFR-007"
          category: "usability"
          description: "Mock scripts must be easy to configure and extend"
          measurable_target: "All configuration via documented MOCK_* environment variables"
clarifications:
    - date: "2026-01-16"
      question: "Should all binary commands be tested, or just the 60% target in SC-001?"
      answer: "100% coverage - All 35+ commands must have at least one E2E test"
      applied_to: "success_criteria.SC-001, requirements.functional (new FR-015)"
    - date: "2026-01-16"
      question: "Is 5 minute E2E suite time budget sufficient for 35+ commands?"
      answer: "Keep 5 minutes - tests should be fast with mocks, use parallelization"
      applied_to: "requirements.non_functional.NFR-001"
    - date: "2026-01-16"
      question: "Do worktree/dag commands need special test isolation beyond E2EEnv?"
      answer: "Same E2EEnv - tests create git state as needed in temp repo"
      applied_to: "assumptions"
    - date: "2026-01-16"
      question: "How should network-dependent commands (update, doctor) be tested?"
      answer: "Mock network responses - inject mock version/connectivity data via env vars"
      applied_to: "edge_cases, key_entities.MockScript"
success_criteria:
    measurable_outcomes:
        - id: "SC-001"
          description: "E2E test coverage reaches 100% of CLI commands"
          metric: "Percentage of CLI commands with at least one E2E test"
          target: "100% (all 35+ commands have at least one E2E test)"
        - id: "SC-002"
          description: "All 6 exit codes are verified by dedicated tests"
          metric: "Number of exit codes with E2E test coverage"
          target: "6 out of 6 exit codes tested"
        - id: "SC-003"
          description: "Mock-opencode.sh enables OpenCode agent testing"
          metric: "OpenCode agent preset can be tested end-to-end"
          target: "At least 1 workflow test uses --agent opencode"
        - id: "SC-004"
          description: "Implement command flags have test coverage"
          metric: "Number of implement flags with E2E tests"
          target: "At least 5 of 8 implement flags tested (--phases, --tasks, --phase, --from-phase, --from-task)"
        - id: "SC-005"
          description: "Run command has basic E2E coverage"
          metric: "Number of run command E2E tests"
          target: "At least 5 run command tests covering -s, -p, -t, -i, -a flags"
        - id: "SC-006"
          description: "No API calls made during E2E tests"
          metric: "API calls detected in E2E test runs"
          target: "0 API calls in all E2E test runs"
        - id: "SC-007"
          description: "E2E tests pass in CI without special configuration"
          metric: "CI pass rate for E2E tests"
          target: "100% pass rate in GitHub Actions"
key_entities:
    - name: "E2EEnv"
      description: "Test environment struct that provides isolated environment for E2E tests"
      attributes:
        - "TempDir: isolated temporary directory for all file operations"
        - "MockPath: PATH containing only mock binaries"
        - "OriginalEnv: backup of original environment for cleanup"
        - "CallLog: path to mock call log file"
    - name: "MockScript"
      description: "Shell script that simulates CLI agent behavior (claude, opencode)"
      attributes:
        - "MOCK_EXIT_CODE: configurable exit code"
        - "MOCK_DELAY: simulated execution delay"
        - "MOCK_ARTIFACT_DIR: directory for generated artifacts"
        - "MOCK_CALL_LOG: path for logging invocations"
        - "MOCK_RESPONSE_FILE: custom response content"
        - "MOCK_VERSION_INFO: mock version data for update command testing"
        - "MOCK_NETWORK_STATUS: mock connectivity status for doctor command testing"
    - name: "TestData"
      description: "Pre-defined YAML artifacts used as mock responses"
      attributes:
        - "spec.yaml: valid specification artifact"
        - "plan.yaml: valid implementation plan artifact"
        - "tasks.yaml: valid task breakdown artifact"
        - "checklist.yaml: valid checklist artifact"
        - "analysis.yaml: valid analysis artifact"
edge_cases:
    - scenario: "Mock binary not found in PATH during test setup"
      expected_behavior: "Test fails with clear error message before any commands run"
    - scenario: "Multiple E2E tests run in parallel"
      expected_behavior: "Each test has isolated temp directory; no interference between tests"
    - scenario: "Mock script receives unexpected slash command"
      expected_behavior: "Mock exits with code 0 and logs the unrecognized command"
    - scenario: "MOCK_DELAY exceeds test timeout"
      expected_behavior: "Test times out with clear timeout error, not hang"
    - scenario: "Artifact directory does not exist when mock tries to write"
      expected_behavior: "Mock creates directory before writing artifact"
    - scenario: "spec.yaml contains special characters or very long content"
      expected_behavior: "Mock handles content correctly without corruption"
    - scenario: "Git repo initialization fails in temp directory"
      expected_behavior: "Test fails with error about git setup, not obscure later error"
    - scenario: "Network-dependent command (update, doctor) runs in isolated test"
      expected_behavior: "Command uses mock version/connectivity data from MOCK_* env vars instead of real network calls"
assumptions:
    - "Go 1.25+ is available in test environment"
    - "Git is available in test environment"
    - "Bash is available for mock scripts (POSIX shell compatible)"
    - "Temp directories can be created and cleaned up reliably"
    - "Environment variables can be modified within test scope"
    - "Mock scripts have execute permission"
    - "Test data files are valid YAML that passes artifact validation"
    - "Worktree/dag tests use same E2EEnv isolation; tests create necessary git state (branches, worktrees) within temp repo"
constraints:
    - "Mock scripts must work on Linux and macOS (POSIX compatible)"
    - "No external dependencies beyond Go, git, and bash"
    - "Tests must not require network access"
    - "Tests must not require sudo/root permissions"
    - "Tests must clean up all temp files even on failure"
    - "Test execution time must be bounded (no infinite loops/hangs)"
out_of_scope:
    - "Windows support for mock scripts (documented limitation)"
    - "Performance benchmarking of autospec commands"
    - "Integration testing with real LLM APIs"
    - "GUI/TUI testing"
    - "Documentation generation testing"
    - "Mock support for agents other than claude and opencode"
    - "Stress testing or load testing"
    - "Security vulnerability scanning"
_meta:
    version: "1.0.0"
    generator: "autospec"
    generator_version: "autospec 0.9.0"
    created: "2026-01-16T18:20:34Z"
    artifact_type: "spec"
