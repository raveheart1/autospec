feature:
  branch: "102-e2e-mock-testing"
  created: "2026-01-16"
  status: "Draft"
  input: "create a comprehensive spec for .dev/tasks/e2e-test-coverage-analysis.md"

user_stories:
  - id: "US-001"
    title: "Safety Verification Tests"
    priority: "P1"
    as_a: "developer running E2E tests"
    i_want: "tests that verify no real API calls are made and environment is properly isolated"
    so_that: "I can run tests without incurring API costs or affecting production systems"
    why_this_priority: "Non-negotiable safety requirement - real API calls must never occur in tests"
    independent_test: "Run E2E tests and verify ANTHROPIC_API_KEY is not accessible and only mock binaries are in PATH"
    acceptance_scenarios:
      - given: "E2E test environment is initialized"
        when: "any E2E test runs"
        then: "ANTHROPIC_API_KEY and OPENAI_API_KEY are not in environment"
      - given: "E2E test environment is initialized"
        when: "PATH is checked"
        then: "only mock binaries (mock-claude.sh, mock-opencode.sh) are accessible, not real CLI binaries"
      - given: "E2E test runs any command"
        when: "command writes state files"
        then: "state files are written to temp directory, not ~/.autospec/ or ~/.config/autospec/"
      - given: "E2E test runs git operations"
        when: "git commands execute"
        then: "operations use temp git repo with GIT_CONFIG_GLOBAL=/dev/null"

  - id: "US-002"
    title: "OpenCode Agent Mock Support"
    priority: "P1"
    as_a: "developer testing OpenCode agent workflows"
    i_want: "a mock-opencode.sh script that simulates OpenCode CLI behavior"
    so_that: "I can test autospec workflows with the OpenCode agent preset without real API calls"
    why_this_priority: "OpenCode is a supported agent preset with zero E2E coverage - critical gap"
    independent_test: "Run autospec with --agent opencode and verify mock-opencode.sh is invoked and generates correct artifacts"
    acceptance_scenarios:
      - given: "mock-opencode.sh exists and is executable"
        when: "autospec runs with --agent opencode"
        then: "mock-opencode.sh is invoked instead of real opencode binary"
      - given: "mock-opencode.sh receives a specify command"
        when: "command contains /autospec.specify"
        then: "mock generates valid spec.yaml artifact"
      - given: "MOCK_EXIT_CODE environment variable is set"
        when: "mock-opencode.sh runs"
        then: "mock exits with the specified exit code"
      - given: "MOCK_CALL_LOG environment variable is set"
        when: "mock-opencode.sh runs"
        then: "command invocation is logged to specified file"

  - id: "US-003"
    title: "Exit Code Verification"
    priority: "P1"
    as_a: "developer testing error handling"
    i_want: "E2E tests that verify all 6 documented exit codes (0-5)"
    so_that: "I can confirm autospec returns correct exit codes for all scenarios"
    why_this_priority: "Exit codes are part of the CLI contract - must be verified for automation/scripting"
    independent_test: "Run tests for each exit code scenario and verify correct code is returned"
    acceptance_scenarios:
      - given: "workflow completes successfully"
        when: "autospec exits"
        then: "exit code is 0"
      - given: "artifact validation fails"
        when: "autospec exits"
        then: "exit code is 1"
      - given: "retry limit is exhausted"
        when: "autospec exits"
        then: "exit code is 2"
      - given: "invalid arguments are provided"
        when: "autospec exits"
        then: "exit code is 3"
      - given: "required dependency is missing"
        when: "autospec exits"
        then: "exit code is 4"
      - given: "command times out"
        when: "autospec exits"
        then: "exit code is 5"

  - id: "US-004"
    title: "Implement Command Flag Testing"
    priority: "P1"
    as_a: "developer using implement command variants"
    i_want: "E2E tests covering all implement command execution modes"
    so_that: "I can be confident phase-based and task-based execution work correctly"
    why_this_priority: "implement has the most flags (8+) with zero flag coverage - highest gap"
    independent_test: "Run implement with each execution mode flag and verify correct behavior"
    acceptance_scenarios:
      - given: "tasks.yaml exists with multiple phases"
        when: "autospec implement --phases runs"
        then: "each phase runs in a separate session"
      - given: "tasks.yaml exists with phase 2"
        when: "autospec implement --phase 2 runs"
        then: "only phase 2 tasks execute"
      - given: "tasks.yaml exists and previous run was interrupted"
        when: "autospec implement --from-phase 2 runs"
        then: "execution resumes from phase 2"
      - given: "tasks.yaml exists with multiple tasks"
        when: "autospec implement --tasks runs"
        then: "each task runs in a separate session"
      - given: "tasks.yaml exists with task T003"
        when: "autospec implement --from-task T003 runs"
        then: "execution resumes from task T003"

  - id: "US-005"
    title: "Run Command Coverage"
    priority: "P1"
    as_a: "developer using the run command"
    i_want: "E2E tests covering all run command stage selection flags"
    so_that: "I can verify stage selection and optional stages work correctly"
    why_this_priority: "run command has zero E2E coverage despite being primary entry point"
    independent_test: "Run with various flag combinations and verify correct stages execute"
    acceptance_scenarios:
      - given: "constitution exists"
        when: "autospec run -spti 'feature' executes"
        then: "specify, plan, tasks, and implement all run in order"
      - given: "constitution exists"
        when: "autospec run -a 'feature' executes"
        then: "behavior is identical to -spti"
      - given: "constitution exists"
        when: "autospec run -itps 'feature' executes"
        then: "stages run in correct order (specify, plan, tasks, implement) regardless of flag order"
      - given: "spec.yaml exists"
        when: "autospec run -p executes"
        then: "only plan stage runs"
      - given: "no spec.yaml exists"
        when: "autospec run -p executes"
        then: "command fails with error mentioning 'spec'"

  - id: "US-006"
    title: "Phase Ordering Property Tests"
    priority: "P2"
    as_a: "developer maintaining autospec"
    i_want: "property-based tests that verify flag order independence"
    so_that: "I can be confident that flag permutations don't affect execution order"
    why_this_priority: "Important for CLI robustness but core flag tests are higher priority"
    independent_test: "Generate random flag permutations and verify same artifacts are created"
    acceptance_scenarios:
      - given: "any permutation of flags -s, -p, -t, -i"
        when: "autospec run with that permutation executes"
        then: "stages always execute in order: specify, plan, tasks, implement"
      - given: "subset of flags specified (e.g., -sp)"
        when: "autospec run executes"
        then: "only specified stages run and only their artifacts are created"
      - given: "a stage is requested without its prerequisite"
        when: "autospec run executes"
        then: "error message mentions the missing prerequisite"

  - id: "US-007"
    title: "Interactive Stage Mock Support"
    priority: "P2"
    as_a: "developer testing optional workflow stages"
    i_want: "mock support for clarify, checklist, and analyze commands"
    so_that: "I can test all workflow stages without real API calls"
    why_this_priority: "Optional stages have zero E2E coverage but are less frequently used"
    independent_test: "Run clarify, checklist, analyze and verify mock generates appropriate artifacts"
    acceptance_scenarios:
      - given: "spec.yaml exists"
        when: "autospec clarify runs"
        then: "mock-claude.sh detects /autospec.clarify and updates spec.yaml with clarifications"
      - given: "spec.yaml exists"
        when: "autospec checklist runs"
        then: "mock-claude.sh detects /autospec.checklist and generates checklist.yaml"
      - given: "spec.yaml, plan.yaml, tasks.yaml exist"
        when: "autospec analyze runs"
        then: "mock-claude.sh detects /autospec.analyze and generates analysis.yaml"

  - id: "US-008"
    title: "Global Flag Testing"
    priority: "P2"
    as_a: "developer using global autospec flags"
    i_want: "E2E tests for global flags like --max-retries, --agent, --config"
    so_that: "I can verify global flags work correctly across all commands"
    why_this_priority: "Global flags affect all commands but have zero coverage"
    independent_test: "Run commands with global flags and verify they take effect"
    acceptance_scenarios:
      - given: "command is run with --max-retries 5"
        when: "mock returns failure"
        then: "command retries up to 5 times before failing"
      - given: "command is run with --agent opencode"
        when: "command executes"
        then: "mock-opencode.sh is invoked instead of mock-claude.sh"
      - given: "custom config file exists"
        when: "command is run with --config ./custom.yml"
        then: "settings from custom.yml are used"
      - given: "command is run with --skip-preflight"
        when: "command executes"
        then: "preflight checks are skipped"

  - id: "US-009"
    title: "Error Scenario Coverage"
    priority: "P2"
    as_a: "developer handling error cases"
    i_want: "E2E tests for various error scenarios"
    so_that: "I can verify error messages are clear and exit codes are correct"
    why_this_priority: "Error handling affects user experience but success paths are higher priority"
    independent_test: "Trigger each error scenario and verify error message and exit code"
    acceptance_scenarios:
      - given: "implement command is run"
        when: "tasks.yaml does not exist"
        then: "exit code is 1 and error message mentions 'tasks'"
      - given: "implement --phase 99 is run"
        when: "phase 99 does not exist"
        then: "exit code is 1 and error message mentions invalid phase"
      - given: "agent binary is removed from PATH"
        when: "any workflow command runs"
        then: "exit code is 4 and error message mentions missing binary"
      - given: "config file has invalid YAML"
        when: "any command runs"
        then: "exit code is 1 and error message mentions parse error"

  - id: "US-010"
    title: "Utility Command Testing"
    priority: "P3"
    as_a: "developer using utility commands"
    i_want: "E2E tests for status, history, clean, view, and ck commands"
    so_that: "I can verify utility commands work correctly"
    why_this_priority: "Utility commands are less critical than workflow commands"
    independent_test: "Run each utility command and verify output/behavior"
    acceptance_scenarios:
      - given: "a spec exists"
        when: "autospec status runs"
        then: "status shows current spec state"
      - given: "commands have been run"
        when: "autospec history runs"
        then: "history shows recent command executions"
      - given: "a spec.yaml exists"
        when: "autospec view spec runs"
        then: "spec contents are displayed"
      - given: "a valid YAML file exists"
        when: "autospec ck file.yaml runs"
        then: "validation passes"

requirements:
  functional:
    - id: "FR-001"
      description: "MUST never invoke real claude or opencode CLI binaries during E2E tests"
      testable: true
      acceptance_criteria: "E2EEnv isolates PATH to only include mock binaries; verification test confirms real binaries are not accessible"

    - id: "FR-002"
      description: "MUST sanitize ANTHROPIC_API_KEY, OPENAI_API_KEY, and similar environment variables in E2E tests"
      testable: true
      acceptance_criteria: "E2EEnv removes all API keys from environment; verification test confirms no API keys are accessible"

    - id: "FR-003"
      description: "MUST use temp directories for all state files (retry.json, config.yml, history.json)"
      testable: true
      acceptance_criteria: "E2EEnv sets XDG_CONFIG_HOME, HOME, AUTOSPEC_STATE_DIR to temp directory; no writes to real home directory"

    - id: "FR-004"
      description: "MUST isolate git operations with GIT_CONFIG_GLOBAL=/dev/null and temp repos"
      testable: true
      acceptance_criteria: "E2EEnv creates fresh git repo in temp dir with minimal config; no interaction with real .git/"

    - id: "FR-005"
      description: "MUST provide mock-opencode.sh script that mimics OpenCode CLI behavior"
      testable: true
      acceptance_criteria: "mock-opencode.sh exists, is executable, parses -m flag, generates artifacts, supports MOCK_* variables"

    - id: "FR-006"
      description: "MUST verify all 6 exit codes (0-5) in E2E tests"
      testable: true
      acceptance_criteria: "Dedicated test cases exist for each exit code with appropriate mock configurations"

    - id: "FR-007"
      description: "MUST test all implement command execution modes (--phases, --tasks, --phase N, --from-phase, --from-task)"
      testable: true
      acceptance_criteria: "E2E tests exist for each execution mode with verified correct behavior"

    - id: "FR-008"
      description: "MUST test run command stage selection flags (-s, -p, -t, -i, -a) and optional stage flags (-n, -r, -l, -z)"
      testable: true
      acceptance_criteria: "E2E tests exist for stage selection with verified correct stage execution order"

    - id: "FR-009"
      description: "MUST enhance mock-claude.sh to support /autospec.clarify, /autospec.checklist, /autospec.analyze"
      testable: true
      acceptance_criteria: "Mock script generates appropriate artifacts for each slash command"

    - id: "FR-010"
      description: "MUST test global flags --max-retries, --agent, --config, --skip-preflight across commands"
      testable: true
      acceptance_criteria: "E2E tests verify each global flag affects command behavior correctly"

    - id: "FR-011"
      description: "SHOULD test phase ordering invariant (flag order does not affect execution order)"
      testable: true
      acceptance_criteria: "Property-based test verifies -spti and -itps produce same results"

    - id: "FR-012"
      description: "SHOULD test error scenarios with clear error messages and correct exit codes"
      testable: true
      acceptance_criteria: "E2E tests for missing prerequisites, invalid flags, missing binaries verify error messages"

    - id: "FR-013"
      description: "MAY test utility commands (status, history, clean, view, ck)"
      testable: true
      acceptance_criteria: "E2E tests exist for utility commands with basic functionality verification"

    - id: "FR-014"
      description: "MUST make tests pass with 'make test && make fmt && make lint && make build' all exit 0"
      testable: true
      acceptance_criteria: "All quality gates pass after implementation"

  non_functional:
    - id: "NFR-001"
      category: "performance"
      description: "E2E tests should complete within CI timeout limits"
      measurable_target: "Individual E2E tests complete in under 30 seconds; full E2E suite in under 5 minutes"

    - id: "NFR-002"
      category: "reliability"
      description: "E2E tests must be deterministic and not flaky"
      measurable_target: "100% pass rate on 10 consecutive runs with no intermittent failures"

    - id: "NFR-003"
      category: "reliability"
      description: "E2E tests must run identically on Linux, macOS, and CI environments"
      measurable_target: "Tests pass on local Linux/macOS and GitHub Actions without modification"

    - id: "NFR-004"
      category: "code_quality"
      description: "Functions must be under 40 lines"
      measurable_target: "No function exceeds 40 lines"

    - id: "NFR-005"
      category: "code_quality"
      description: "Errors must be wrapped with context"
      measurable_target: "All errors use fmt.Errorf('doing X: %w', err) pattern"

    - id: "NFR-006"
      category: "code_quality"
      description: "Tests must use map-based table test pattern"
      measurable_target: "All table tests use map[string]struct pattern"

    - id: "NFR-007"
      category: "usability"
      description: "Mock scripts must be easy to configure and extend"
      measurable_target: "All configuration via documented MOCK_* environment variables"

success_criteria:
  measurable_outcomes:
    - id: "SC-001"
      description: "E2E test coverage increases from 23% to at least 60% of CLI commands"
      metric: "Percentage of CLI commands with at least one E2E test"
      target: "60% or higher (21+ of 35 commands)"

    - id: "SC-002"
      description: "All 6 exit codes are verified by dedicated tests"
      metric: "Number of exit codes with E2E test coverage"
      target: "6 out of 6 exit codes tested"

    - id: "SC-003"
      description: "Mock-opencode.sh enables OpenCode agent testing"
      metric: "OpenCode agent preset can be tested end-to-end"
      target: "At least 1 workflow test uses --agent opencode"

    - id: "SC-004"
      description: "Implement command flags have test coverage"
      metric: "Number of implement flags with E2E tests"
      target: "At least 5 of 8 implement flags tested (--phases, --tasks, --phase, --from-phase, --from-task)"

    - id: "SC-005"
      description: "Run command has basic E2E coverage"
      metric: "Number of run command E2E tests"
      target: "At least 5 run command tests covering -s, -p, -t, -i, -a flags"

    - id: "SC-006"
      description: "No API calls made during E2E tests"
      metric: "API calls detected in E2E test runs"
      target: "0 API calls in all E2E test runs"

    - id: "SC-007"
      description: "E2E tests pass in CI without special configuration"
      metric: "CI pass rate for E2E tests"
      target: "100% pass rate in GitHub Actions"

key_entities:
  - name: "E2EEnv"
    description: "Test environment struct that provides isolated environment for E2E tests"
    attributes:
      - "TempDir: isolated temporary directory for all file operations"
      - "MockPath: PATH containing only mock binaries"
      - "OriginalEnv: backup of original environment for cleanup"
      - "CallLog: path to mock call log file"

  - name: "MockScript"
    description: "Shell script that simulates CLI agent behavior (claude, opencode)"
    attributes:
      - "MOCK_EXIT_CODE: configurable exit code"
      - "MOCK_DELAY: simulated execution delay"
      - "MOCK_ARTIFACT_DIR: directory for generated artifacts"
      - "MOCK_CALL_LOG: path for logging invocations"
      - "MOCK_RESPONSE_FILE: custom response content"

  - name: "TestData"
    description: "Pre-defined YAML artifacts used as mock responses"
    attributes:
      - "spec.yaml: valid specification artifact"
      - "plan.yaml: valid implementation plan artifact"
      - "tasks.yaml: valid task breakdown artifact"
      - "checklist.yaml: valid checklist artifact"
      - "analysis.yaml: valid analysis artifact"

edge_cases:
  - scenario: "Mock binary not found in PATH during test setup"
    expected_behavior: "Test fails with clear error message before any commands run"

  - scenario: "Multiple E2E tests run in parallel"
    expected_behavior: "Each test has isolated temp directory; no interference between tests"

  - scenario: "Mock script receives unexpected slash command"
    expected_behavior: "Mock exits with code 0 and logs the unrecognized command"

  - scenario: "MOCK_DELAY exceeds test timeout"
    expected_behavior: "Test times out with clear timeout error, not hang"

  - scenario: "Artifact directory does not exist when mock tries to write"
    expected_behavior: "Mock creates directory before writing artifact"

  - scenario: "spec.yaml contains special characters or very long content"
    expected_behavior: "Mock handles content correctly without corruption"

  - scenario: "Git repo initialization fails in temp directory"
    expected_behavior: "Test fails with error about git setup, not obscure later error"

assumptions:
  - "Go 1.25+ is available in test environment"
  - "Git is available in test environment"
  - "Bash is available for mock scripts (POSIX shell compatible)"
  - "Temp directories can be created and cleaned up reliably"
  - "Environment variables can be modified within test scope"
  - "Mock scripts have execute permission"
  - "Test data files are valid YAML that passes artifact validation"

constraints:
  - "Mock scripts must work on Linux and macOS (POSIX compatible)"
  - "No external dependencies beyond Go, git, and bash"
  - "Tests must not require network access"
  - "Tests must not require sudo/root permissions"
  - "Tests must clean up all temp files even on failure"
  - "Test execution time must be bounded (no infinite loops/hangs)"

out_of_scope:
  - "Windows support for mock scripts (documented limitation)"
  - "Performance benchmarking of autospec commands"
  - "Integration testing with real LLM APIs"
  - "GUI/TUI testing"
  - "Documentation generation testing"
  - "Mock support for agents other than claude and opencode"
  - "Stress testing or load testing"
  - "Security vulnerability scanning"

_meta:
  version: "1.0.0"
  generator: "autospec"
  generator_version: "autospec 0.9.0"
  created: "2026-01-16T18:20:34Z"
  artifact_type: "spec"
