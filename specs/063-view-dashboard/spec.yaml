feature:
    branch: "063-view-dashboard"
    created: "2025-12-19"
    status: "Completed"
    completed_at: 2025-12-19T21:25:37Z
    input: "create a new 'autospec view' command that will give a dashboard overview of all specs - progress for top (5/n configurable) recent ones, total number of specs in current project, # in progress, # skipped/rejected, # tasks per spec, completed specs section"
user_stories:
    - id: "US-001"
      title: "Dashboard Overview of All Specs"
      priority: "P1"
      as_a: "developer using autospec"
      i_want: "a single command that shows me a dashboard overview of all specs in the project"
      so_that: "I can quickly understand the overall state of my feature development without checking each spec individually"
      why_this_priority: "Core functionality - the main value proposition of the view command"
      independent_test: "Run autospec view in a project with multiple specs and verify dashboard displays"
      acceptance_scenarios:
        - given: "a project with 10 specs in various states"
          when: "I run autospec view"
          then: "I see a dashboard summary showing total specs, recent specs with progress, and categorized counts"
        - given: "a project with no specs directory"
          when: "I run autospec view"
          then: "I see a friendly message indicating no specs found"
    - id: "US-002"
      title: "Recent Specs Progress Section"
      priority: "P1"
      as_a: "developer using autospec"
      i_want: "to see progress for the most recent specs at a glance"
      so_that: "I can focus on actively developed features without scrolling through all specs"
      why_this_priority: "Key differentiator from existing status command - aggregated recent view"
      independent_test: "Run autospec view in a project with multiple specs sorted by modification time"
      acceptance_scenarios:
        - given: "a project with 15 specs"
          when: "I run autospec view with default settings"
          then: "I see progress for the 5 most recently modified specs"
        - given: "a project with 15 specs"
          when: "I run autospec view --limit 10"
          then: "I see progress for the 10 most recently modified specs"
    - id: "US-003"
      title: "Spec Status Categories"
      priority: "P1"
      as_a: "developer using autospec"
      i_want: "to see specs categorized by status (in progress, completed, skipped/rejected)"
      so_that: "I can quickly identify which features need attention and which are done"
      why_this_priority: "Essential for project management and prioritization"
      independent_test: "Create specs with different statuses and verify they appear in correct categories"
      acceptance_scenarios:
        - given: "specs with status Draft, In Progress, Completed, and Rejected"
          when: "I run autospec view"
          then: "I see counts for each category and completed specs are listed in a separate section"
        - given: "a spec with all tasks completed"
          when: "I run autospec view"
          then: "the spec appears in the completed section with 100% progress"
    - id: "US-004"
      title: "Tasks Per Spec Summary"
      priority: "P2"
      as_a: "developer using autospec"
      i_want: "to see the number of tasks for each spec in the dashboard"
      so_that: "I can understand the relative size and complexity of different features"
      why_this_priority: "Useful context but not essential for basic dashboard functionality"
      independent_test: "View a spec with tasks.yaml and verify task count displays correctly"
      acceptance_scenarios:
        - given: "a spec with tasks.yaml containing 15 tasks across 4 phases"
          when: "I run autospec view"
          then: "I see the task count (e.g., '8/15 tasks') for that spec"
        - given: "a spec without tasks.yaml"
          when: "I run autospec view"
          then: "I see an indicator that no tasks are defined (e.g., 'no tasks')"
    - id: "US-005"
      title: "Configurable Display Limit"
      priority: "P2"
      as_a: "developer using autospec"
      i_want: "to configure how many recent specs are shown"
      so_that: "I can customize the dashboard to my preferences and screen size"
      why_this_priority: "Configuration flexibility is important but default behavior is sufficient initially"
      independent_test: "Run autospec view with different --limit values and verify correct number shown"
      acceptance_scenarios:
        - given: "a project with 20 specs"
          when: "I run autospec view --limit 3"
          then: "I see only the 3 most recent specs in detail"
        - given: "config file has view_limit: 10"
          when: "I run autospec view without flags"
          then: "I see 10 specs (using config value)"
        - given: "config file has view_limit: 10"
          when: "I run autospec view --limit 5"
          then: "I see 5 specs (flag overrides config)"
requirements:
    functional:
        - id: "FR-001"
          description: "MUST implement a new 'autospec view' command under the util command group"
          testable: true
          acceptance_criteria: "Command is registered, appears in help, and can be executed"
        - id: "FR-002"
          description: "MUST display total number of specs in the current project's specs directory"
          testable: true
          acceptance_criteria: "Count matches actual number of spec directories"
        - id: "FR-003"
          description: "MUST display the top N most recently modified specs with their progress"
          testable: true
          acceptance_criteria: "Specs sorted by modification time, N configurable via --limit flag"
        - id: "FR-004"
          description: "MUST show spec categorization counts (in progress, completed, skipped/rejected)"
          testable: true
          acceptance_criteria: "Counts accurately reflect spec statuses from spec.yaml files"
        - id: "FR-005"
          description: "MUST display task counts for each spec (completed/total) when tasks.yaml exists"
          testable: true
          acceptance_criteria: "Task progress shown as 'X/Y tasks' format"
        - id: "FR-006"
          description: "MUST include a completed specs section showing all specs with 100% task completion"
          testable: true
          acceptance_criteria: "Specs with all tasks completed listed in dedicated section"
        - id: "FR-007"
          description: "SHOULD support --limit flag to configure number of recent specs displayed"
          testable: true
          acceptance_criteria: "Flag accepts positive integer, defaults to 5"
        - id: "FR-008"
          description: "SHOULD support view_limit configuration option in config file"
          testable: true
          acceptance_criteria: "Config value used when --limit flag not provided"
        - id: "FR-009"
          description: "MUST handle edge cases gracefully (no specs, missing files, parse errors)"
          testable: true
          acceptance_criteria: "User-friendly messages for each error case, no panics"
        - id: "FR-010"
          description: "MUST pass all quality gates: make test, make fmt, make lint, and make build"
          testable: true
          acceptance_criteria: "All commands exit 0; no test failures, format changes, lint errors, or build failures"
    non_functional:
        - id: "NFR-001"
          category: "performance"
          description: "Dashboard SHOULD render within 2 seconds for projects with up to 100 specs"
          measurable_target: "Execution time < 2 seconds measured via time command"
        - id: "NFR-002"
          category: "usability"
          description: "Dashboard output MUST be readable in standard 80-column terminal"
          measurable_target: "No line wrapping at 80 columns for summary information"
        - id: "NFR-003"
          category: "code_quality"
          description: "All functions must be under 40 lines; extract helpers for complex logic"
          measurable_target: "No function exceeds 40 lines excluding comments"
        - id: "NFR-004"
          category: "code_quality"
          description: "All errors must be wrapped with context using fmt.Errorf(\"doing X: %w\", err)"
          measurable_target: "Zero bare 'return err' statements in new code"
        - id: "NFR-005"
          category: "code_quality"
          description: "Tests must use map-based table-driven pattern with t.Parallel()"
          measurable_target: "All new test functions use map[string]struct pattern and call t.Parallel()"
        - id: "NFR-006"
          category: "code_quality"
          description: "Accept interfaces, return concrete types"
          measurable_target: "Function signatures follow interface-in, concrete-out pattern where applicable"
success_criteria:
    measurable_outcomes:
        - id: "SC-001"
          description: "Users can view project-wide spec status in a single command"
          metric: "Command output includes all required sections"
          target: "100% of dashboard sections render correctly"
        - id: "SC-002"
          description: "Dashboard provides accurate spec counts and progress metrics"
          metric: "Counts match manual verification"
          target: "Zero discrepancies between dashboard and actual spec states"
        - id: "SC-003"
          description: "Command execution is fast enough for interactive use"
          metric: "Time from invocation to output"
          target: "Under 2 seconds for projects with up to 100 specs"
key_entities:
    - name: "SpecSummary"
      description: "Aggregated information about a single spec for dashboard display"
      attributes:
        - "name (branch name)"
        - "status (Draft, In Progress, Completed, Rejected)"
        - "task progress (completed/total)"
        - "last modified timestamp"
        - "artifacts present (spec, plan, tasks)"
    - name: "DashboardStats"
      description: "Project-wide statistics for the dashboard header"
      attributes:
        - "total specs count"
        - "in progress count"
        - "completed count"
        - "skipped/rejected count"
edge_cases:
    - scenario: "Empty specs directory"
      expected_behavior: "Display friendly message 'No specs found in specs/' without error"
    - scenario: "Spec directory without spec.yaml"
      expected_behavior: "Skip the directory, do not include in counts"
    - scenario: "Malformed spec.yaml or tasks.yaml"
      expected_behavior: "Log warning, show spec as 'parse error', continue with other specs"
    - scenario: "Spec with tasks.yaml but 0 tasks defined"
      expected_behavior: "Show '0 tasks' rather than progress bar"
    - scenario: "Config has invalid view_limit value (negative or zero)"
      expected_behavior: "Use default value of 5 with warning message"
assumptions:
    - "Specs are stored in directories under the configured specs_dir (default: specs/)"
    - "Each spec directory follows naming convention: NNN-feature-name"
    - "spec.yaml contains a feature.status field indicating spec state"
    - "tasks.yaml follows the existing TasksYAML schema in internal/validation"
    - "Modification time for sorting is based on the most recently modified file in each spec directory"
    - "The existing validation.GetTaskStats function can be reused for task progress calculation"
constraints:
    - "Must integrate with existing CLI command structure under internal/cli/util"
    - "Must use existing configuration loading via internal/config"
    - "Must follow established error handling patterns with clierrors package"
    - "Output format must be plain text suitable for terminal display"
    - "Must not add new external dependencies"
out_of_scope:
    - "Interactive dashboard with live updates"
    - "Graphical progress bars (text-based indicators only)"
    - "Export to JSON/CSV formats (can be added later)"
    - "Filtering specs by name pattern or date range"
    - "Integration with external project management tools"
    - "Web-based dashboard interface"
_meta:
    version: "1.0.0"
    generator: "autospec"
    generator_version: "autospec dev"
    created: "2025-12-19T21:11:54Z"
    artifact_type: "spec"
